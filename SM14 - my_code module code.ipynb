{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_datasets(path):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mne\n",
    "    from mne.io import read_raw_edf\n",
    "    list_files = os.listdir(path=path)\n",
    "    \n",
    "    extension = '.edf'\n",
    "    index = 0\n",
    "    list_dataset = []\n",
    "    for file in list_files:\n",
    "        if extension in list_files[index]:\n",
    "            list_dataset.append(list_files[index])\n",
    "        index += 1\n",
    "\n",
    "    list_load_dataset = []\n",
    "    for n_file in range(0, len(list_dataset)):\n",
    "        dataset = read_raw_edf(list_dataset[n_file], preload=True)\n",
    "        list_load_dataset.append(dataset)\n",
    "        \n",
    "    return list_load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencing scheme, montage, filtering, annotating, segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preliminary_steps(raw_datasets):\n",
    "    import mne\n",
    "    pre_processed_datasets = []\n",
    "    \n",
    "    # re-referencing the data to 'CQ_CMS', 'CQ_DRL'\n",
    "    \n",
    "    for dataset in raw_datasets:\n",
    "        dataset.set_eeg_reference(ref_channels=['CQ_CMS', 'CQ_DRL'])\n",
    "    \n",
    "    # selecting only the electrodes of interest\n",
    "    \n",
    "        include_channels = ['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4']\n",
    "        \n",
    "        for channels in dataset.ch_names:\n",
    "            if channels not in include_channels:\n",
    "                dataset.drop_channels(channels)\n",
    "            \n",
    "    # setting montage \n",
    "    \n",
    "        dataset.set_montage(mne.channels.make_standard_montage(\"standard_1020\"))\n",
    "        \n",
    "    \n",
    "    # high pass filter to remove slow drifts\n",
    "    \n",
    "        dataset = dataset.copy().filter(l_freq=0.16, h_freq=None)\n",
    "\n",
    "        \n",
    "    # notch filter to remove powerline noise\n",
    "    \n",
    "        freqs = (50, 100)\n",
    "        dataset = dataset.copy().notch_filter(freqs=freqs)\n",
    "        \n",
    "    \n",
    "    # annotating raw data\n",
    "    \n",
    "        # we can create multiple annotations at the same time\n",
    "        # total time -> 60s + 5 x(20s) = 160 s\n",
    "\n",
    "        start_time = 60\n",
    "        delay = 1\n",
    "        dur = 20\n",
    "        len_event = 7 \n",
    "        tot_time = start_time + delay \n",
    "\n",
    "\n",
    "        start_push = [tot_time + 0*dur, tot_time + 1*dur, tot_time+2*dur, tot_time+3*dur, tot_time+4*dur]\n",
    "        start_relax = [tot_time + 0*dur+10, tot_time + 1*dur+10, tot_time+2*dur+10, tot_time+3*dur+10, tot_time+4*dur+10]\n",
    "\n",
    "        \n",
    "        push_annotations = mne.Annotations(onset=start_push, duration=[len_event]*5, description=[\"Push\"]*5, orig_time=dataset.info['meas_date'])\n",
    "        relax_annotations = mne.Annotations (onset=start_relax, duration=[len_event]*5, description=[\"Relax\"]*5, orig_time=dataset.info['meas_date'])\n",
    "\n",
    "        dataset.set_annotations(push_annotations+relax_annotations)\n",
    "        \n",
    "    \n",
    "    # creating events from annotations \n",
    "    \n",
    "    \n",
    "    \n",
    "    # now that we have annotations, we will tranfer them into events\n",
    "    # this is needed to be able to then create epochs\n",
    "        events_from_annot, event_dict = mne.events_from_annotations(dataset)\n",
    "        pre_processed_datasets.append(dataset)\n",
    "    return pre_processed_datasets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs(pre_processed_datasets):\n",
    "    import mne\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(pre_processed_datasets[0])\n",
    "    delay = 0.5\n",
    "    \n",
    "    baseline = (0.5, 0.5)\n",
    "    event_dict = {\"Push\" : 1, \"Relax\" : 2}\n",
    "    # not sure what to set it to; resting state activity looks super noisy \n",
    "    epochs_all = mne.Epochs(pre_processed_datasets[0], events=events_from_annot, event_id = event_dict, baseline = baseline, tmin = 0.5, tmax = (10-delay), preload = True, reject_by_annotation=False)\n",
    "    # reject by annotation argument passed\n",
    "    \n",
    "    for dataset in pre_processed_datasets:\n",
    "        \n",
    "        baseline = (0.5, 0.5)\n",
    "        epochs = mne.Epochs(dataset, events=events_from_annot, event_id = event_dict, baseline = baseline, tmin = 0.5, tmax = (10-delay), preload = True, reject_by_annotation=False)\n",
    "        epochs_all = mne.concatenate_epochs([epochs_all, epochs])\n",
    "        \n",
    "    epochs_all.drop([0,1,2,3,4,5,6,7,8,9])\n",
    "    return epochs_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard data cleaning approach (ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ica(cleaned_epochs):\n",
    "    import numpy as np\n",
    "    from mne.preprocessing import ICA\n",
    "    ica = ICA(n_components=14, random_state=97, method=\"fastica\")\n",
    "    ica.fit(cleaned_epochs)\n",
    "\n",
    "    ica.plot_sources(cleaned_epochs, show_scrollbars=False)\n",
    "    ica.plot_components()\n",
    "    \n",
    "    components = np.arange(0,14)\n",
    "    for component in components:\n",
    "        ica.plot_properties(cleaned_epochs, picks=component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated data cleaning approach (Autoreject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_epochs(epochs_all):\n",
    "        \n",
    "    import autoreject\n",
    "    import numpy as np \n",
    "    from autoreject import AutoReject\n",
    "    from autoreject import get_rejection_threshold\n",
    "    import mne\n",
    "    from autoreject import compute_thresholds  \n",
    "    import matplotlib.pyplot as plt \n",
    "    from autoreject import set_matplotlib_defaults \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # creating a random list of parameters which will be modified by learning \n",
    "    # n_interpolates are the ρ values that we would like autoreject to try\n",
    "    # consensus_percs are the κ values that autoreject will try \n",
    "    \n",
    "    n_interpolates = np.array([1, 4, 32])\n",
    "    consensus_percs = np.linspace(0, 1.0, 11)\n",
    "    \n",
    "    # specifying the channel type\n",
    "    info = epochs_all.info\n",
    "    picks = mne.pick_types(info, meg=False, eeg=True, stim=False, eog=False, ecg=False, emg=False, ref_meg='auto', misc=False, resp=False, chpi=False, exci=False, ias=False, syst=False, seeg=False, dipole=False, gof=False, bio=False, ecog=False, fnirs=False, include=(), exclude='bads', selection=None)\n",
    "    \n",
    "    # initiating the algorithm \n",
    "    \n",
    "    ar = AutoReject(n_interpolates, consensus_percs, picks=picks,\n",
    "                thresh_method='random_search', random_state=42)\n",
    "\n",
    "    \n",
    "    # need to fit the ar first in order to then transform the epochs which can be repaired \n",
    "    \n",
    "    ar.fit(epochs_all)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    reject_log = ar.get_reject_log(epochs_all)\n",
    "    reject_log.plot()\n",
    "    reject_log.plot_epochs(epochs_all)\n",
    "    \n",
    "    # repairing epochs where possible \n",
    "    \n",
    "    cleaned_epochs = ar.transform(epochs_all)\n",
    "\n",
    "\n",
    "    # get a dictionary of rejection thresholds\n",
    "    threshes = compute_thresholds(epochs_all, picks=picks, method='random_search',\n",
    "                              random_state=42, augment=False,\n",
    "                              verbose='progressbar')\n",
    "  \n",
    "    values = list(threshes.values())\n",
    "    plt.bar(epochs_all.ch_names, values, color ='maroon')\n",
    "\n",
    "    plt.xlabel(\"Channel name\")\n",
    "    plt.ylabel(\"Threshold value [V]\")\n",
    "    plt.title(\"Different threshold values across the sensors\")\n",
    "    plt.show()\n",
    "    \n",
    "    return cleaned_epochs, threshes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_frequency_matrices(low_band, high_band, num, n_epochs, channels, n_samples, epochs):\n",
    "    import numpy as np\n",
    "    import mne\n",
    "    # frequencies of interest\n",
    "    freqs = np.logspace(*np.log10([low_band, high_band]), num=num)\n",
    "\n",
    "    # empty matrices \n",
    "    power_data = np.zeros(shape=(n_epochs,channels, num, n_samples))\n",
    "\n",
    "    itc_data = np.zeros(shape=(n_epochs,channels, num, n_samples))\n",
    "\n",
    "    # number of epochs per class\n",
    "    instances = np.arange(0,n_epochs,1)\n",
    "    \n",
    "    for i in instances:\n",
    "        n_cycles = freqs / 2.\n",
    "\n",
    "        power_data_single, itc_data_single = mne.time_frequency.tfr_morlet(epochs[i], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "    \n",
    "        power_data[i,:,:,:] = power_data_single.data\n",
    "        itc_data[i,:,:,:] = itc_data_single.data\n",
    "    \n",
    "    \n",
    "    return  freqs, power_data, itc_data, instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_channels(frequency_vectors_class1,frequency_vectors_class2,epochs_object,n_epochs):\n",
    "    import numpy as np \n",
    "    import mne\n",
    "    # calculating the mean over the frequencies between specified Hz band \n",
    "    \n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1.mean(axis=2)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2.mean(axis=2)\n",
    "    \n",
    "    # calculating mean over all instances\n",
    "    \n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1_mean.mean(axis=0)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2_mean.mean(axis=0)\n",
    "    \n",
    "    # getting a channel list to be able to loop over it\n",
    "    \n",
    "    channel_list = epochs_object.ch_names\n",
    "    \n",
    "    frequency_difference_vectors = np.zeros((14,769))\n",
    "\n",
    "    for item in range(0, len(channel_list)):\n",
    "        frequency_difference_vectors[item,:] = frequency_vectors_class1_mean[item] - frequency_vectors_class2_mean[item]\n",
    "        \n",
    "    # calculate absolute values of differences\n",
    "\n",
    "    frequency_difference_vectors = abs(frequency_difference_vectors)\n",
    "    \n",
    "    # creating an empty vector that will hold averaged difference values in 10 intervals \n",
    "    averaged_frequency_differences_per_channel_in_intervals = np.zeros((14,11))\n",
    "    slices = np.arange(0,769-76, 76)\n",
    "    slices = slices.tolist()\n",
    "    \n",
    "    # populating this empty vector with actual averaged differences per interval\n",
    "    for item in range(0, len(channel_list)):\n",
    "        for interval in slices:\n",
    "            averaged_frequency_differences_per_channel_in_intervals[item, slices.index(interval)] = frequency_difference_vectors[item,interval:interval+76].mean()\n",
    "            \n",
    "    # calculating differences across all of the intervals per channel\n",
    "    frequency_channel_differences = np.zeros(14)\n",
    "    for channel in range(0, len(channel_list)):\n",
    "        frequency_channel_differences[channel] = averaged_frequency_differences_per_channel_in_intervals[channel].mean()\n",
    "        \n",
    "    # creating a copy of these differences and converting it to a list\n",
    "    # so that we will be able to do proper indexing \n",
    "    \n",
    "    frequency_channel_differences_indexing = frequency_channel_differences.copy()\n",
    "    frequency_channel_differences_indexing = frequency_channel_differences_indexing.tolist()\n",
    "    \n",
    "    # creating an empty matrix that will hold maximum values of channels with biggest differences\n",
    "    max_channel_differences = []\n",
    "    \n",
    "    # populating the max values \n",
    "    \n",
    "    max1 = frequency_channel_differences.max()\n",
    "    max_channel_differences =  np.append(max_channel_differences, max1, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max1))\n",
    "    max2 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max2, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max2))\n",
    "    max3 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max3, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max3))\n",
    "    max4 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max4, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max4))\n",
    "    max5 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max5, axis=None)                                      \n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max5))\n",
    "    max6 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max6, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max6))\n",
    "                                          \n",
    "    # getting indices of those max values to be able to know which channels to include\n",
    "                                          \n",
    "    channel_indices = []\n",
    "    for item in max_channel_differences:\n",
    "        index = frequency_channel_differences_indexing.index(item)\n",
    "        channel_indices = np.append(index, channel_indices, axis=None)\n",
    "    \n",
    "    # converting the channel indices into a list so it can be used as an index in input vector slicing \n",
    "    channel_indices = channel_indices.tolist()\n",
    "    # converting floats to integers \n",
    "    channel_indices = [int(channel_indices) for channel_indices in channel_indices]\n",
    "    \n",
    "    # creating frequency input vectors with engineered channels\n",
    "    import my_code \n",
    "\n",
    "    class1_channel_engineered = my_code.select_channels(channel_indices, frequency_vectors_class1, n_epochs, 769)\n",
    "    class2_channel_engineered = my_code.select_channels(channel_indices, frequency_vectors_class2, n_epochs, 769)\n",
    "    \n",
    "    return class1_channel_engineered, class2_channel_engineered, channel_indices, frequency_channel_differences_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_time(frequency_vectors_class1,frequency_vectors_class2,epochs_object):\n",
    "        # calculating the mean over the frequencies between specified Hz band \n",
    "    import numpy as np \n",
    "    import mne\n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1.mean(axis=2)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2.mean(axis=2)\n",
    "    \n",
    "    # calculating mean over all instances\n",
    "    \n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1_mean.mean(axis=0)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2_mean.mean(axis=0)\n",
    "    \n",
    "    # getting a channel list to be able to loop over it\n",
    "    \n",
    "    channel_list = epochs_object.ch_names\n",
    "    \n",
    "    frequency_difference_vectors = np.zeros((14,769))\n",
    "\n",
    "    for item in range(0, len(channel_list)):\n",
    "        frequency_difference_vectors[item,:] = frequency_vectors_class1_mean[item] - frequency_vectors_class2_mean[item]\n",
    "        \n",
    "    # calculate absolute values of differences\n",
    "\n",
    "    frequency_difference_vectors = abs(frequency_difference_vectors)\n",
    "    \n",
    "    # creating an empty vector that will hold averaged difference values in 10 intervals \n",
    "    averaged_frequency_differences_per_channel_in_intervals = np.zeros((14,10))\n",
    "    slices = np.arange(0,769-76, 76)\n",
    "    slices = slices.tolist()\n",
    "    \n",
    "    # populating this empty vector with actual averaged differences per interval\n",
    "    for item in range(0, len(channel_list)):\n",
    "        for interval in slices:\n",
    "            averaged_frequency_differences_per_channel_in_intervals[item, slices.index(interval)] = frequency_difference_vectors[item,interval:interval+76].mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # collapsing channels so that we work with all the 10 intervals\n",
    "    \n",
    "    frequency_differences_collapsed_channels = averaged_frequency_differences_per_channel_in_intervals.mean(axis=0)\n",
    "    \n",
    "    # getting the interval with biggest differences \n",
    "    \n",
    "    max_time_differences = frequency_differences_collapsed_channels.max()\n",
    "    \n",
    "    # copying and converting into a list\n",
    "    frequency_differences_collapsed_channels_list = frequency_differences_collapsed_channels.copy()\n",
    "    frequency_differences_collapsed_channels_list=  frequency_differences_collapsed_channels_list.tolist()\n",
    "    max_time_difference_index = frequency_differences_collapsed_channels_list.index(max_time_differences)\n",
    "    \n",
    "    # now, based on the interval with biggest time differences\n",
    "    # we will crop our input vectors such that\n",
    "    # we will take the interval with biggest time differences\n",
    "    # (which takes up 1/11 of data)\n",
    "    # plus 230 backwards and 230 forward so that we are using 70% of timepoints in total\n",
    "    if max_time_difference_index > 4 and max_time_difference_index < 6:\n",
    "        \n",
    "        class1_channel_engineered = frequency_vectors_class1[:,:, :, slices[max_time_difference_index]-230:slices[max_time_difference_index]+76+230]\n",
    "        class2_channel_engineered = frequency_vectors_class2[:,:, :, slices[max_time_difference_index]-230:slices[max_time_difference_index]+76+230]\n",
    "        \n",
    "    elif max_time_difference_index < 4:\n",
    "    \n",
    "        class1_channel_engineered = frequency_vectors_class1[:,:, :, slices[max_time_difference_index]-slices[max_time_difference_index]:slices[max_time_difference_index]+76+230+(230-slices[max_time_difference_index])]\n",
    "        class2_channel_engineered = frequency_vectors_class2[:,:, :, slices[max_time_difference_index]-slices[max_time_difference_index]:slices[max_time_difference_index]+76+230+(230-slices[max_time_difference_index])]\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        class1_channel_engineered = frequency_vectors_class1[:,:, :, slices[max_time_difference_index]-230-(230-(760-(slices[max_time_difference_index]+76))):760]\n",
    "        class2_channel_engineered = frequency_vectors_class2[:,:, :, slices[max_time_difference_index]-230-(230-(760-(slices[max_time_difference_index]+76))):760]\n",
    "    \n",
    "   \n",
    "    \n",
    "    return class1_channel_engineered, class2_channel_engineered, max_time_difference_index, averaged_frequency_differences_per_channel_in_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_channels_time(frequency_vectors_class1,frequency_vectors_class2,epochs_object,n_epochs):\n",
    "    import numpy as np \n",
    "    import mne\n",
    "    import my_code\n",
    "    # calculating the mean over the frequencies between specified Hz band \n",
    "    \n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1.mean(axis=2)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2.mean(axis=2)\n",
    "    \n",
    "    # calculating mean over all instances\n",
    "    \n",
    "    frequency_vectors_class1_mean = frequency_vectors_class1_mean.mean(axis=0)\n",
    "    frequency_vectors_class2_mean = frequency_vectors_class2_mean.mean(axis=0)\n",
    "    \n",
    "    # getting a channel list to be able to loop over it\n",
    "    \n",
    "    channel_list = epochs_object.ch_names\n",
    "    \n",
    "    frequency_difference_vectors = np.zeros((14,769))\n",
    "\n",
    "    for item in range(0, len(channel_list)):\n",
    "        frequency_difference_vectors[item,:] = frequency_vectors_class1_mean[item] - frequency_vectors_class2_mean[item]\n",
    "        \n",
    "    # calculate absolute values of differences\n",
    "\n",
    "    frequency_difference_vectors = abs(frequency_difference_vectors)\n",
    "    \n",
    "    # creating an empty vector that will hold averaged difference values in 10 intervals \n",
    "    averaged_frequency_differences_per_channel_in_intervals = np.zeros((14,11))\n",
    "    slices = np.arange(0,769-76, 76)\n",
    "    slices = slices.tolist()\n",
    "    \n",
    "    # populating this empty vector with actual averaged differences per interval\n",
    "    for item in range(0, len(channel_list)):\n",
    "        for interval in slices:\n",
    "            averaged_frequency_differences_per_channel_in_intervals[item, slices.index(interval)] = frequency_difference_vectors[item,interval:interval+76].mean()\n",
    "            \n",
    "    # calculating differences across all of the intervals per channel\n",
    "    frequency_channel_differences = np.zeros(14)\n",
    "    for channel in range(0, len(channel_list)):\n",
    "        frequency_channel_differences[channel] = averaged_frequency_differences_per_channel_in_intervals[channel].mean()\n",
    "        \n",
    "    # creating a copy of these differences and converting it to a list\n",
    "    # so that we will be able to do proper indexing \n",
    "    \n",
    "    frequency_channel_differences_indexing = frequency_channel_differences.copy()\n",
    "    frequency_channel_differences_indexing = frequency_channel_differences_indexing.tolist()\n",
    "    \n",
    "    # creating an empty matrix that will hold maximum values of channels with biggest differences\n",
    "    max_channel_differences = []\n",
    "    \n",
    "    # populating the max values \n",
    "    \n",
    "    max1 = frequency_channel_differences.max()\n",
    "    max_channel_differences =  np.append(max_channel_differences, max1, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max1))\n",
    "    max2 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max2, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max2))\n",
    "    max3 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max3, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max3))\n",
    "    max4 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max4, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max4))\n",
    "    max5 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max5, axis=None)                                      \n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max5))\n",
    "    max6 = frequency_channel_differences.max()\n",
    "    max_channel_differences = np.append(max_channel_differences, max6, axis=None)\n",
    "    frequency_channel_differences = np.delete(frequency_channel_differences, np.where(frequency_channel_differences == max6))\n",
    "                                          \n",
    "    # getting indices of those max values to be able to know which channels to include\n",
    "                                          \n",
    "    channel_indices = []\n",
    "    for item in max_channel_differences:\n",
    "        index = frequency_channel_differences_indexing.index(item)\n",
    "        channel_indices = np.append(index, channel_indices, axis=None)\n",
    "    \n",
    "    # converting the channel indices into a list so it can be used as an index in input vector slicing \n",
    "    channel_indices = channel_indices.tolist()\n",
    "    # converting floats to integers \n",
    "    channel_indices = [int(channel_indices) for channel_indices in channel_indices]\n",
    "    \n",
    "    \n",
    "    # now, we will be calculating biggest time differences out of the selected channels \n",
    "    # first, select the channels with biggest differences\n",
    "    frequencies_cropped_channels_and_time = averaged_frequency_differences_per_channel_in_intervals[channel_indices,:]\n",
    "    frequencies_cropped_channels_and_time_collapsed_channels = frequencies_cropped_channels_and_time.mean(axis=0)\n",
    "    \n",
    "    # next, calculate biggest time difference and its index\n",
    "    max_time_differences = frequencies_cropped_channels_and_time_collapsed_channels.max()\n",
    "    frequencies_cropped_channels_and_time_collapsed_channels_list = frequencies_cropped_channels_and_time_collapsed_channels.copy()\n",
    "    frequencies_cropped_channels_and_time_collapsed_channels_list = frequencies_cropped_channels_and_time_collapsed_channels_list.tolist()\n",
    "    max_time_difference_index = frequencies_cropped_channels_and_time_collapsed_channels_list.index(max_time_differences)\n",
    "    \n",
    "    # select channels from original frequency vectors\n",
    "\n",
    "    frequencies_class1_channel_time_engineered = my_code.select_channels(channel_indices, frequency_vectors_class1, n_epochs, 769)\n",
    "    frequencies_class2_channel_time_engineered = my_code.select_channels(channel_indices, frequency_vectors_class2, n_epochs, 769)\n",
    "    \n",
    "    # now, select the timepoints from those channel-engineered frequency vectors\n",
    "    \n",
    "    if max_time_difference_index < 4:\n",
    "    \n",
    "        class1_channel_time_engineered = frequencies_class1_channel_time_engineered[:,:, :, slices[max_time_difference_index]-slices[max_time_difference_index]:slices[max_time_difference_index]+76+230+(230-slices[max_time_difference_index])]\n",
    "        class2_channel_time_engineered = frequencies_class2_channel_time_engineered[:,:, :, slices[max_time_difference_index]-slices[max_time_difference_index]:slices[max_time_difference_index]+76+230+(230-slices[max_time_difference_index])]\n",
    "    \n",
    "    \n",
    "    else: \n",
    "        \n",
    "        class1_channel_time_engineered = frequencies_class1_channel_time_engineered[:,:, :, slices[max_time_difference_index]-230:slices[max_time_difference_index]+76+230]\n",
    "        class2_channel_time_engineered = frequencies_class2_channel_time_engineered[:,:, :, slices[max_time_difference_index]-230:slices[max_time_difference_index]+76+230]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    return class1_channel_time_engineered, class2_channel_time_engineered, max_time_difference_index, channel_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda(class1, class2, n_epochs):\n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "    # average over frequencies in the range \n",
    "    class1=class1.mean(axis=2)\n",
    "    class2=class2.mean(axis=2)\n",
    "    \n",
    "    # flattening the input vectors\n",
    "    \n",
    "    class1 = class1.reshape(n_epochs, -1)\n",
    "    class2 = class2.reshape(n_epochs, -1)\n",
    "    \n",
    "    # concatenating into one input vector\n",
    "    \n",
    "    input_data = np.concatenate((class1, class2), axis = 0)\n",
    "    \n",
    "    # creating labels\n",
    "    \n",
    "    push = np.ones(shape=(n_epochs))\n",
    "    relax = np.zeros(shape=(n_epochs))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    \n",
    "    # Now, we run the alpha data through LDA\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    scores = []\n",
    "    all_data = input_data\n",
    "    all_data_train = input_data.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "# Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    clf = Pipeline([('standardscaler', StandardScaler()), ('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"LDA Classification accuracy:\", np.mean(scores))\n",
    "    \n",
    "    \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_for_ready_input(input_data, labels):\n",
    "    \n",
    "    # Now, we run the alpha data through LDA\n",
    "    import numpy as np\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    input_data = StandardScaler().fit_transform(input_data)\n",
    "    \n",
    "    scores = []\n",
    "    all_data = input_data\n",
    "    all_data_train = input_data.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "    import numpy as np\n",
    "    # Printing the results\n",
    "    print(\"LDA Classification accuracy:\", scores.mean())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(class1, class2, n_epochs):\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    # average over frequencies in the range \n",
    "    class1=class1.mean(axis=2)\n",
    "    class2=class2.mean(axis=2)\n",
    "    \n",
    "    # concatenating into one input vector\n",
    "    \n",
    "    input_data = np.concatenate((class1, class2), axis = 0)\n",
    "    \n",
    "    # creating labels\n",
    "    \n",
    "    push = np.ones(shape=(n_epochs))\n",
    "    relax = np.zeros(shape=(n_epochs))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    # flattening the input data\n",
    "    \n",
    "    input_data = input_data.reshape(len(input_data), -1)\n",
    "\n",
    "    # normalizing the input data \n",
    "    input_data = StandardScaler().fit_transform(input_data)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Define an SVM classifier (SVC) with a linear kernel\n",
    "    clf = SVC(C=1, kernel='poly', degree=1)\n",
    "\n",
    "    \n",
    "    # specifing cv \n",
    "    cv = ShuffleSplit(len(input_data), 10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # classifying the data\n",
    "    scores_full = cross_val_score(clf, input_data, labels, cv=cv, n_jobs=1)\n",
    "    \n",
    "    # printing results\n",
    "    print(\"SVM Classification score: %s (std. %s)\" % (np.mean(scores_full), np.std(scores_full)))\n",
    "    \n",
    "    return scores_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_for_ready_input(input_data, n_epochs):\n",
    "    import numpy as np\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "    # creating labels\n",
    "    \n",
    "    push = np.ones(shape=(n_epochs))\n",
    "    relax = np.zeros(shape=(n_epochs))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    # normalizing the input data\n",
    "    \n",
    "    input_data = StandardScaler().fit_transform(input_data)\n",
    "\n",
    "\n",
    "    # Define an SVM classifier (SVC) with a linear kernel\n",
    "    clf = SVC(C=1, kernel='poly', degree=1)\n",
    "\n",
    "    \n",
    "    # specifing cv \n",
    "    cv = ShuffleSplit(len(input_data), 10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # classifying the data\n",
    "    scores_full = cross_val_score(clf, input_data, labels, cv=cv, n_jobs=1)\n",
    "    \n",
    "    # printing results\n",
    "    print(\"SVM Classification score: %s (std. %s)\" % (np.mean(scores_full), np.std(scores_full)))\n",
    "    \n",
    "    return scores_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_for_DL_input(epoch_object, n_epochs, train_size, validation_size, test_size):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    # we will start off with extracting data from our epoch object \n",
    "    \n",
    "    \n",
    "    push_data = epoch_object[\"Push\"].get_data()\n",
    "    relax_data = epoch_object[\"Relax\"].get_data()\n",
    "    # we then concatenate data for the two classes\n",
    "    features = np.concatenate((push_data, relax_data), axis=0)\n",
    "    # and flatten them\n",
    "    features = features.reshape(n_epochs*2,-1)\n",
    "    \n",
    "    # creating labels\n",
    "    labels = np.concatenate((np.zeros(n_epochs), np.ones(n_epochs)))\n",
    "    \n",
    "    # normalizing features\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features)\n",
    "    features = scaler.transform(features)\n",
    "    \n",
    "    #shuffling the data\n",
    "    labels = labels.reshape(n_epochs*2,1)\n",
    "    features_and_labels = np.concatenate((features, labels), axis= 1)\n",
    "    \n",
    "    df = pd.DataFrame(data=features_and_labels)\n",
    "    df = shuffle(df)\n",
    "    # isolating labels\n",
    "    labels = df[32270]\n",
    "    labels = labels.tolist()\n",
    "    labels = np.array(labels)\n",
    "    features = df.iloc[:, 0:32270]\n",
    "    features = features.to_numpy()\n",
    "    \n",
    "    \n",
    "    # train-test partition\n",
    "    \n",
    "    train_samples = features[:train_size]\n",
    "    validation_samples = train_samples[:validation_size]\n",
    "    train_samples = train_samples[-validation_size:]\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "    train_labels = train_labels[-validation_size:]\n",
    "\n",
    "    test_samples = features[test_size:]\n",
    "    test_labels = labels[test_size:]\n",
    "\n",
    "    # reshaping the data to be suitable for LSTM input\n",
    "    \n",
    "    train_samples = train_samples.reshape(validation_size, 1,  32270)\n",
    "    validation_samples = validation_samples.reshape(validation_size, 1,  32270)\n",
    "    \n",
    "    return  train_samples, validation_samples, train_labels, validation_labels, test_samples, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_frequencies_for_DL_input(class1data, class2data, n_epochs, train_size, validation_size, test_size):\n",
    " \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    # preparing frequency vectors for input to LSTM \n",
    "    # first off, we have frequency time vectors for each class separately\n",
    "    # each of shape (115, 14, 20, 769)\n",
    "    \n",
    "    # we start by flattening the vectors\n",
    "    \n",
    "    class1_power_DL_input = class1data.mean(axis=2)\n",
    "    class1_power_DL_input = class1_power_DL_input.reshape(n_epochs,-1)\n",
    "    class2_power_DL_input = class2data.mean(axis=2)\n",
    "    class2_power_DL_input =  class2_power_DL_input.reshape(n_epochs,-1)\n",
    "    \n",
    "    # we then concatenate data for the two classes\n",
    "    \n",
    "    features = np.concatenate((class1_power_DL_input, class2_power_DL_input), axis=0)\n",
    "    \n",
    "    # creating a label vector\n",
    "    \n",
    "    labels = np.concatenate((np.zeros(n_epochs), np.ones(n_epochs)))\n",
    "    \n",
    "    # normalizing the feature values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features)\n",
    "    features = scaler.transform(features)\n",
    "    \n",
    "    # shuffling of the data\n",
    "    # first, concatenate features with labels\n",
    "    \n",
    "    labels = labels.reshape(n_epochs*2,1)\n",
    "    features_and_labels = np.concatenate((features, labels), axis= 1)\n",
    "    \n",
    "    # now, shuffle\n",
    "    df = pd.DataFrame(data=features_and_labels)\n",
    "    df = shuffle(df)\n",
    "    \n",
    "    # isolate labels from features\n",
    "    indexing = class2_power_DL_input.shape[1]\n",
    "    labels = df[indexing]\n",
    "    labels = labels.tolist()\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    features = df.iloc[:, 0:indexing]\n",
    "    features = features.to_numpy()\n",
    "    \n",
    "    # train-test partition\n",
    "    \n",
    "    train_samples = features[:train_size]\n",
    "    validation_samples = train_samples[:validation_size]\n",
    "\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "\n",
    "    test_samples = features[-test_size:]\n",
    "    test_labels = labels[-test_size:]\n",
    "\n",
    "    # reshaping the data to be suitable for LSTM input\n",
    "    \n",
    "    train_samples = train_samples.reshape(train_size, 1,  class2_power_DL_input.shape[1])\n",
    "    test_samples = test_samples.reshape(test_size, 1,  class2_power_DL_input.shape[1])\n",
    "    \n",
    "    return  train_samples, validation_samples, train_labels, validation_labels, test_samples, test_labels\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM(train_samples, validation_samples, train_labels, validation_labels, test_samples, test_labels):\n",
    "    \n",
    "    def build_model():\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "\n",
    "        from tensorflow.keras.layers import Dense\n",
    "        from tensorflow.keras.layers import LSTM\n",
    "        tf.keras.backend.clear_session()\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "        from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(1,train_samples.shape[2]), return_sequences=True))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(LSTM(32, return_sequences=True))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        from tensorflow.keras.optimizers  import SGD\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "            print(c[epoch % 4], end='')\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\b', end='')\n",
    "\n",
    "    import numpy as np, tensorflow as tf\n",
    "    K = 3\n",
    "    num_val_samples = len(train_samples) // K\n",
    "    num_epochs = 20\n",
    "    all_loss_histories = []\n",
    "    all_acc_histories = []\n",
    "    all_training_accuracies_histories = []\n",
    "    all_training_losses_histories = []\n",
    "    for i in range(K):\n",
    "        print('processing fold', i)\n",
    "    \n",
    "        # Prepare the validation data: data from partition i\n",
    "        a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "        val_data = train_samples[a : b]\n",
    "        val_targets = train_labels[a : b]\n",
    "    \n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate([train_samples[:a], train_samples[b:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([train_labels[:a], train_labels[b:]], axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        modelsmall = build_model()\n",
    "    \n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = modelsmall.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=16, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "        loss_history = history.history['val_loss']\n",
    "        all_loss_histories.append(loss_history)\n",
    "    \n",
    "        acc_history = history.history[\"val_accuracy\"]\n",
    "        all_acc_histories.append(acc_history)\n",
    "    \n",
    "        training_loss_history = history.history['loss']\n",
    "        all_training_losses_histories.append(training_loss_history)\n",
    "    \n",
    "        training_accuracy_history = history.history[\"accuracy\"]\n",
    "        all_training_accuracies_histories.append(training_accuracy_history)\n",
    "\n",
    "    # this object holds all losses per epoch, per validation fold\n",
    "\n",
    "    average_loss_history_small = [np.mean([x[i] for x in all_loss_histories])for i in range(20)]\n",
    "    average_acc_history_small = [np.mean([x[i] for x in all_acc_histories])for i in range(20)]\n",
    "    average_training_loss_history_small = [np.mean([x[i] for x in all_training_losses_histories])for i in range(20)]\n",
    "    average_training_accuracy_history_small = [np.mean([x[i] for x in all_training_accuracies_histories])for i in range(20)]\n",
    "\n",
    "\n",
    "    def plot_loss(start, data):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data) + 1), data[start:])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "    plot_loss(0, average_loss_history_small)\n",
    "\n",
    "    def plot_losses(start, data1, data2):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data1) + 1), data1[start:], color=\"b\", label=\"Averaged Validation loss\")\n",
    "        plt.plot(range(start+1, len(data2)+1), data2[start:], color=\"r\", label=\"Averaged Training loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation & Training Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_losses(0, average_loss_history_small, average_training_loss_history_small)\n",
    "\n",
    "    average_acc_history_small = np.array(average_acc_history_small, np.float)\n",
    "    average_training_accuracy_history_small = np.array(average_training_accuracy_history_small, np.float)\n",
    "\n",
    "    print(average_acc_history_small.mean())\n",
    "    print(average_training_accuracy_history_small.mean())\n",
    "    return all_loss_histories, all_acc_histories, all_training_accuracies_histories, all_training_losses_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CONV1D(train_samples, validation_samples, train_labels, validation_labels, test_samples, test_labels):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_model():\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "\n",
    "        from tensorflow.keras.layers import Dense\n",
    "        from tensorflow.keras.layers import LSTM\n",
    "        tf.keras.backend.clear_session()\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "        from tensorflow.keras import models, layers, optimizers\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(layers.Conv1D(512, 1, activation='relu', input_shape=(1,train_samples.shape[2])))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(1))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv1D(512, 1,  activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(1))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv1D(128, 1,activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "        from tensorflow.keras.optimizers  import SGD\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "            print(c[epoch % 4], end='')\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\b', end='')\n",
    "\n",
    "    import numpy as np, tensorflow as tf\n",
    "    K = 3\n",
    "    num_val_samples = len(train_samples) // K\n",
    "    num_epochs = 20\n",
    "    all_loss_histories = []\n",
    "    all_acc_histories = []\n",
    "    all_training_accuracies_histories = []\n",
    "    all_training_losses_histories = []\n",
    "    for i in range(K):\n",
    "        print('processing fold', i)\n",
    "    \n",
    "        # Prepare the validation data: data from partition i\n",
    "        a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "        val_data = train_samples[a : b]\n",
    "        val_targets = train_labels[a : b]\n",
    "    \n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate([train_samples[:a], train_samples[b:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([train_labels[:a], train_labels[b:]], axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        modelsmall = build_model()\n",
    "\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = modelsmall.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=16, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "        loss_history = history.history['val_loss']\n",
    "        all_loss_histories.append(loss_history)\n",
    "\n",
    "        acc_history = history.history[\"val_accuracy\"]\n",
    "        all_acc_histories.append(acc_history)\n",
    "\n",
    "        training_loss_history = history.history['loss']\n",
    "        all_training_losses_histories.append(training_loss_history)\n",
    "\n",
    "        training_accuracy_history = history.history[\"accuracy\"]\n",
    "        all_training_accuracies_histories.append(training_accuracy_history)\n",
    "\n",
    "    # this object holds all losses per epoch, per validation fold\n",
    "\n",
    "    average_loss_history_small = [np.mean([x[i] for x in all_loss_histories])for i in range(20)]\n",
    "    average_acc_history_small = [np.mean([x[i] for x in all_acc_histories])for i in range(20)]\n",
    "    average_training_loss_history_small = [np.mean([x[i] for x in all_training_losses_histories])for i in range(20)]\n",
    "    average_training_accuracy_history_small = [np.mean([x[i] for x in all_training_accuracies_histories])for i in range(20)]\n",
    "\n",
    "\n",
    "    def plot_loss(start, data):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data) + 1), data[start:])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "    plot_loss(0, average_loss_history_small)\n",
    "\n",
    "    def plot_losses(start, data1, data2):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data1) + 1), data1[start:], color=\"b\", label=\"Averaged Validation loss\")\n",
    "        plt.plot(range(start+1, len(data2)+1), data2[start:], color=\"r\", label=\"Averaged Training loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation & Training Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_losses(0, average_loss_history_small, average_training_loss_history_small)\n",
    "\n",
    "    average_acc_history_small = np.array(average_acc_history_small, np.float)\n",
    "    average_training_accuracy_history_small = np.array(average_training_accuracy_history_small, np.float)\n",
    "\n",
    "    print(average_acc_history_small.mean())\n",
    "    print(average_training_accuracy_history_small.mean())\n",
    "\n",
    "    return all_loss_histories, all_acc_histories, all_training_accuracies_histories, all_training_losses_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CONV2D(n_epochs, train_size, validation_size, test_size):\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set(color_codes=True)\n",
    "    # selecting only push spectrograms (ending with .png)\n",
    "    import os\n",
    "    path = os.getcwd()\n",
    "    path\n",
    "\n",
    "    path = \"C:\\\\Users\\\\domin\\\\Desktop\\\\Thesis Coding\\\\Datafiles from Giuseppe\"\n",
    "    list_files = os.listdir(path=path)\n",
    "\n",
    "    extension = 'p.png'\n",
    "    index = 0\n",
    "    list_dataset = []\n",
    "    for file in list_files:\n",
    "        if extension in list_files[index]:\n",
    "            list_dataset.append(list_files[index])\n",
    "        index += 1\n",
    "\n",
    "    list_dataset\n",
    "\n",
    "    list_of_images_push = np.zeros((n_epochs, 150, 150, 3))\n",
    "    for item in list_dataset:\n",
    "        image = cv2.imread(item)\n",
    "        image.resize((150, 150, 3))\n",
    "        list_of_images_push[list_dataset.index(item)] = image\n",
    "\n",
    "\n",
    "    list_of_images_push.shape\n",
    "\n",
    "    path = \"C:\\\\Users\\\\domin\\\\Desktop\\\\Thesis Coding\\\\Datafiles from Giuseppe\"\n",
    "    list_files = os.listdir(path=path)\n",
    "\n",
    "    extension = 'r.png'\n",
    "    index = 0\n",
    "    list_dataset2 = []\n",
    "    for file in list_files:\n",
    "        if extension in list_files[index]:\n",
    "            list_dataset2.append(list_files[index])\n",
    "        index += 1\n",
    "\n",
    "    list_dataset2\n",
    "\n",
    "    list_of_images_relax = np.zeros((n_epochs, 150, 150, 3))\n",
    "    for item in list_dataset2:\n",
    "        image = cv2.imread(item)\n",
    "        image.resize((150, 150, 3))\n",
    "        list_of_images_relax[list_dataset2.index(item)] = image\n",
    "\n",
    "\n",
    "    # flattening the width and height \n",
    "\n",
    "    list_of_images_relax_resized = list_of_images_relax.reshape(n_epochs,150*150,3)\n",
    "\n",
    "    list_of_images_relax_resized.shape\n",
    "\n",
    "    list_of_images_push_resized = list_of_images_push.reshape(n_epochs,150*150,3)\n",
    "\n",
    "    # converting to floats\n",
    "\n",
    "    list_of_images_push_resized = list_of_images_push_resized.astype(\"float32\")\n",
    "    list_of_images_relax_resized = list_of_images_relax_resized.astype(\"float32\")\n",
    "\n",
    "    # dividing by 255 (max value of channel depth dimension)\n",
    "    # to make sure classifier does not receive large values\n",
    "\n",
    "    list_of_images_push_resized /= 255\n",
    "    list_of_images_relax_resized /= 255\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    # we then concatenate data for the two classes\n",
    "\n",
    "    features = np.concatenate((list_of_images_push_resized, list_of_images_relax_resized ), axis=0)\n",
    "    features.shape\n",
    "\n",
    "    labels = np.concatenate((np.zeros(n_epochs), np.ones(n_epochs)))\n",
    "\n",
    "    import random\n",
    "\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    features = features[indices]\n",
    "\n",
    "\n",
    "    labels = labels[indices]\n",
    "    instances = np.arange(0,230,1)\n",
    "    instances\n",
    "\n",
    "    for i in instances:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(features[i])\n",
    "        features[i] = scaler.transform(features[i])\n",
    "\n",
    "    features.shape\n",
    "\n",
    "    features_new = features.reshape((n_epochs*2, 150, 150, 3))\n",
    "\n",
    "\n",
    "    features = features_new\n",
    "\n",
    "\n",
    "\n",
    "    # train-test partition\n",
    "\n",
    "    train_samples = features[:train_size]\n",
    "    validation_samples = train_samples[:validation_size]\n",
    "    train_samples = train_samples[-validation_size:]\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "    train_labels = train_labels[-validation_size:]\n",
    "\n",
    "    test_samples = features[test_size:]\n",
    "    test_labels = labels[test_size:]\n",
    "\n",
    "\n",
    "\n",
    "    def build_model():\n",
    "    # similarly to LSTM, input is 3D\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "        from keras.utils import np_utils\n",
    "\n",
    "        # to calculate accuracy\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras import models\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            input_shape=(150, 150, 3)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "            print(c[epoch % 4], end='')\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\b', end='')\n",
    "\n",
    "    import numpy as np, tensorflow as tf\n",
    "    K = 3\n",
    "    num_val_samples = len(train_samples) // K\n",
    "    num_epochs = 20\n",
    "    all_loss_histories = []\n",
    "    all_acc_histories = []\n",
    "    all_training_accuracies_histories = []\n",
    "    all_training_losses_histories = []\n",
    "    for i in range(K):\n",
    "        print('processing fold', i)\n",
    "\n",
    "        # Prepare the validation data: data from partition i\n",
    "        a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "        val_data = train_samples[a : b]\n",
    "        val_targets = train_labels[a : b]\n",
    "\n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate([train_samples[:a], train_samples[b:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([train_labels[:a], train_labels[b:]], axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        modelsmall = build_model()\n",
    "\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = modelsmall.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=16, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "        loss_history = history.history['val_loss']\n",
    "        all_loss_histories.append(loss_history)\n",
    "\n",
    "        acc_history = history.history[\"val_accuracy\"]\n",
    "        all_acc_histories.append(acc_history)\n",
    "\n",
    "        training_loss_history = history.history['loss']\n",
    "        all_training_losses_histories.append(training_loss_history)\n",
    "\n",
    "        training_accuracy_history = history.history[\"accuracy\"]\n",
    "        all_training_accuracies_histories.append(training_accuracy_history)\n",
    "    # this object holds all losses per epoch, per validation fold\n",
    "\n",
    "\n",
    "    average_loss_history_small = [np.mean([x[i] for x in all_loss_histories])for i in range(20)]\n",
    "    average_acc_history_small = [np.mean([x[i] for x in all_acc_histories])for i in range(20)]\n",
    "    average_training_loss_history_small = [np.mean([x[i] for x in all_training_losses_histories])for i in range(20)]\n",
    "    average_training_accuracy_history_small = [np.mean([x[i] for x in all_training_accuracies_histories])for i in range(20)]\n",
    "\n",
    "\n",
    "    def plot_loss(start, data):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data) + 1), data[start:])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "    plot_loss(0, average_loss_history_small)\n",
    "\n",
    "    def plot_losses(start, data1, data2):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(range(start + 1, len(data1) + 1), data1[start:], color=\"b\", label=\"Averaged Validation loss\")\n",
    "        plt.plot(range(start+1, len(data2)+1), data2[start:], color=\"r\", label=\"Averaged Training loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation & Training Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_losses(0, average_loss_history_small, average_training_loss_history_small)\n",
    "\n",
    "    average_acc_history_small = np.array(average_acc_history_small, np.float)\n",
    "    average_training_accuracy_history_small = np.array(average_training_accuracy_history_small, np.float)\n",
    "\n",
    "    print(average_acc_history_small.mean())\n",
    "    print(average_training_accuracy_history_small.mean())\n",
    "\n",
    "    return all_loss_histories, all_acc_histories, all_training_accuracies_histories, all_training_losses_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CONV1D(train_samples, validation_samples, train_labels, validation_labels, test_samples, test_labels):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_model():\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "\n",
    "        from tensorflow.keras.layers import Dense\n",
    "        from tensorflow.keras.layers import LSTM\n",
    "        tf.keras.backend.clear_session()\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential\n",
    "        from tensorflow.keras import models, layers, optimizers\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(layers.Conv1D(512, 1, activation='relu', input_shape=(1,train_samples.shape[2])))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(1))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv1D(512, 1,  activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(1))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv1D(128, 1,activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "        from tensorflow.keras.optimizers  import SGD\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "            print(c[epoch % 4], end='')\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\b', end='')\n",
    "\n",
    "    import numpy as np, tensorflow as tf\n",
    "    K = 3\n",
    "    num_val_samples = len(train_samples) // K\n",
    "    num_epochs = 2\n",
    "    all_loss_histories = []\n",
    "    all_acc_histories = []\n",
    "    all_training_accuracies_histories = []\n",
    "    all_training_losses_histories = []\n",
    "    for i in range(K):\n",
    "        print('processing fold', i)\n",
    "    \n",
    "        # Prepare the validation data: data from partition i\n",
    "        a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "        val_data = train_samples[a : b]\n",
    "        val_targets = train_labels[a : b]\n",
    "    \n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate([train_samples[:a], train_samples[b:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([train_labels[:a], train_labels[b:]], axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        modelsmall = build_model()\n",
    "\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = modelsmall.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=16, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "        loss_history = history.history['val_loss']\n",
    "        all_loss_histories.append(loss_history)\n",
    "\n",
    "        acc_history = history.history[\"val_accuracy\"]\n",
    "        all_acc_histories.append(acc_history)\n",
    "\n",
    "        training_loss_history = history.history['loss']\n",
    "        all_training_losses_histories.append(training_loss_history)\n",
    "\n",
    "        training_accuracy_history = history.history[\"accuracy\"]\n",
    "        all_training_accuracies_histories.append(training_accuracy_history)\n",
    "\n",
    "    # this object holds all losses per epoch, per validation fold\n",
    "\n",
    "    average_loss_history_small = [np.mean([x[i] for x in all_loss_histories])for i in range(num_epochs)]\n",
    "    average_acc_history_small = [np.mean([x[i] for x in all_acc_histories])for i in range(num_epochs)]\n",
    "    average_training_loss_history_small = [np.mean([x[i] for x in all_training_losses_histories])for i in range(num_epochs)]\n",
    "    average_training_accuracy_history_small = [np.mean([x[i] for x in all_training_accuracies_histories])for i in range(num_epochs)]\n",
    "\n",
    "\n",
    "    average_acc_history_small = np.array(average_acc_history_small, np.float)\n",
    "    average_training_accuracy_history_small = np.array(average_training_accuracy_history_small, np.float)\n",
    "\n",
    "    print(average_acc_history_small.mean())\n",
    "    print(average_training_accuracy_history_small.mean())\n",
    "    \n",
    "    \n",
    "    results = modelsmall.evaluate(test_samples, test_labels)\n",
    "    print(\"Test accuracy of this model was\", results[1])\n",
    "    return all_loss_histories, all_acc_histories, all_training_accuracies_histories, all_training_losses_histories, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CONV2D(n_epochs, train_size, validation_size, test_size):\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set(color_codes=True)\n",
    "    # selecting only push spectrograms (ending with .png)\n",
    "    import os\n",
    "    path = os.getcwd()\n",
    "    path\n",
    "\n",
    "    path = \"C:\\\\Users\\\\domin\\\\Desktop\\\\Thesis Coding\\\\Datafiles from Giuseppe\"\n",
    "    list_files = os.listdir(path=path)\n",
    "\n",
    "    extension = 'p.png'\n",
    "    index = 0\n",
    "    list_dataset = []\n",
    "    for file in list_files:\n",
    "        if extension in list_files[index]:\n",
    "            list_dataset.append(list_files[index])\n",
    "        index += 1\n",
    "\n",
    "    list_dataset\n",
    "\n",
    "    list_of_images_push = np.zeros((n_epochs, 150, 150, 3))\n",
    "    for item in list_dataset:\n",
    "        image = cv2.imread(item)\n",
    "        image.resize((150, 150, 3))\n",
    "        list_of_images_push[list_dataset.index(item)] = image\n",
    "\n",
    "\n",
    "    list_of_images_push.shape\n",
    "\n",
    "    path = \"C:\\\\Users\\\\domin\\\\Desktop\\\\Thesis Coding\\\\Datafiles from Giuseppe\"\n",
    "    list_files = os.listdir(path=path)\n",
    "\n",
    "    extension = 'r.png'\n",
    "    index = 0\n",
    "    list_dataset2 = []\n",
    "    for file in list_files:\n",
    "        if extension in list_files[index]:\n",
    "            list_dataset2.append(list_files[index])\n",
    "        index += 1\n",
    "\n",
    "    list_dataset2\n",
    "\n",
    "    list_of_images_relax = np.zeros((n_epochs, 150, 150, 3))\n",
    "    for item in list_dataset2:\n",
    "        image = cv2.imread(item)\n",
    "        image.resize((150, 150, 3))\n",
    "        list_of_images_relax[list_dataset2.index(item)] = image\n",
    "\n",
    "\n",
    "    # flattening the width and height \n",
    "\n",
    "    list_of_images_relax_resized = list_of_images_relax.reshape(n_epochs,150*150,3)\n",
    "\n",
    "    list_of_images_relax_resized.shape\n",
    "\n",
    "    list_of_images_push_resized = list_of_images_push.reshape(n_epochs,150*150,3)\n",
    "\n",
    "    # converting to floats\n",
    "\n",
    "    list_of_images_push_resized = list_of_images_push_resized.astype(\"float32\")\n",
    "    list_of_images_relax_resized = list_of_images_relax_resized.astype(\"float32\")\n",
    "\n",
    "    # dividing by 255 (max value of channel depth dimension)\n",
    "        # to make sure classifier does not receive large values\n",
    "\n",
    "    list_of_images_push_resized /= 255\n",
    "    list_of_images_relax_resized /= 255\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    # we then concatenate data for the two classes\n",
    "\n",
    "    features = np.concatenate((list_of_images_push_resized, list_of_images_relax_resized ), axis=0)\n",
    "    features.shape\n",
    "\n",
    "    labels = np.concatenate((np.zeros(n_epochs), np.ones(n_epochs)))\n",
    "\n",
    "    import random\n",
    "\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    features = features[indices]\n",
    "\n",
    "\n",
    "    labels = labels[indices]\n",
    "    instances = np.arange(0,230,1)\n",
    "    instances\n",
    "\n",
    "    for i in instances:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(features[i])\n",
    "        features[i] = scaler.transform(features[i])\n",
    "\n",
    "    features.shape\n",
    "\n",
    "    features_new = features.reshape((n_epochs*2, 150, 150, 3))\n",
    "\n",
    "\n",
    "    features = features_new\n",
    "\n",
    "\n",
    "\n",
    "    # train-test partition\n",
    "\n",
    "    train_samples = features[:train_size]\n",
    "    validation_samples = train_samples[:validation_size]\n",
    "\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "\n",
    "\n",
    "    test_samples = features[test_size:]\n",
    "    test_labels = labels[test_size:]\n",
    "\n",
    "\n",
    "\n",
    "    def build_model():\n",
    "    # similarly to LSTM, input is 3D\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "        from keras.utils import np_utils\n",
    "\n",
    "        # to calculate accuracy\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras import models\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                input_shape=(150, 150, 3)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "            print(c[epoch % 4], end='')\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\b', end='')\n",
    "\n",
    "    import numpy as np, tensorflow as tf\n",
    "    K = 3\n",
    "    num_val_samples = len(train_samples) // K\n",
    "    num_epochs = 2\n",
    "    all_loss_histories = []\n",
    "    all_acc_histories = []\n",
    "    all_training_accuracies_histories = []\n",
    "    all_training_losses_histories = []\n",
    "    for i in range(K):\n",
    "        print('processing fold', i)\n",
    "\n",
    "        # Prepare the validation data: data from partition i\n",
    "        a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "        val_data = train_samples[a : b]\n",
    "        val_targets = train_labels[a : b]\n",
    "\n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate([train_samples[:a], train_samples[b:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([train_labels[:a], train_labels[b:]], axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        modelsmall = build_model()\n",
    "\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = modelsmall.fit(partial_train_data, partial_train_targets,\n",
    "                                validation_data=(val_data, val_targets),\n",
    "                                epochs=num_epochs, batch_size=16, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "        loss_history = history.history['val_loss']\n",
    "        all_loss_histories.append(loss_history)\n",
    "\n",
    "        acc_history = history.history[\"val_accuracy\"]\n",
    "        all_acc_histories.append(acc_history)\n",
    "\n",
    "        training_loss_history = history.history['loss']\n",
    "        all_training_losses_histories.append(training_loss_history)\n",
    "\n",
    "        training_accuracy_history = history.history[\"accuracy\"]\n",
    "        all_training_accuracies_histories.append(training_accuracy_history)\n",
    "        # this object holds all losses per epoch, per validation fold\n",
    "\n",
    "\n",
    "    average_loss_history_small = [np.mean([x[i] for x in all_loss_histories])for i in range(num_epochs)]\n",
    "    average_acc_history_small = [np.mean([x[i] for x in all_acc_histories])for i in range(num_epochs)]\n",
    "    average_training_loss_history_small = [np.mean([x[i] for x in all_training_losses_histories])for i in range(num_epochs)]\n",
    "    average_training_accuracy_history_small = [np.mean([x[i] for x in all_training_accuracies_histories])for i in range(num_epochs)]\n",
    "\n",
    "\n",
    "\n",
    "    average_acc_history_small = np.array(average_acc_history_small, np.float)\n",
    "    average_training_accuracy_history_small = np.array(average_training_accuracy_history_small, np.float)\n",
    "\n",
    "    print(average_acc_history_small.mean())\n",
    "    print(average_training_accuracy_history_small.mean())\n",
    "\n",
    "    results = modelsmall.evaluate(test_samples, test_labels)\n",
    "    print(\"Test accuracy of this model was\", results[1])\n",
    "    return all_loss_histories, all_acc_histories, all_training_accuracies_histories, all_training_losses_histories, results\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
