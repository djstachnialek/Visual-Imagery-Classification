{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import my_code\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from mne.io import read_raw_edf\n",
    "from mne.preprocessing import ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the raw data (30 files in a list) \n",
    "\n",
    "path = os.getcwd()\n",
    "raw_datasets = my_code.load_all_datasets(path = path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the raw data:\n",
    "# setting the reference electrodes to 'CQ_CMS', 'CQ_DRL'\n",
    "# setting the montage\n",
    "# high pass filter at 0.16 Hz to remove slow drifts (potentially built in in the EEG headset)\n",
    "# no notch-filtering as it is built in in the EEG headset \n",
    "# annotating the raw data so that we have 5 instances of push VI, 5 instances of relax VI per dataset\n",
    "# creating events from annotations \n",
    "\n",
    "pre_processed_datasets = my_code.preliminary_steps(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-616bd2441526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# first, creating a set of epochs based just on one dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mevents_from_annot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents_from_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processed_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mevent_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Push\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Relax\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# creating epochs from the events (starting at 0.5 s of a push/relax instance and ending at 9.5 s of a push/relax instance)\n",
    "\n",
    "# first, creating a set of epochs based just on one dataset\n",
    "\n",
    "events_from_annot, event_dict = mne.events_from_annotations(pre_processed_datasets[0])\n",
    "event_dict = {\"Push\" : 1, \"Relax\" : 2}\n",
    "baseline = (0.5, 0.5)\n",
    "delay = 0.5\n",
    "epochs_all = mne.Epochs(pre_processed_datasets[0], events=events_from_annot, event_id = event_dict, baseline = baseline, tmin = 0.5, tmax = (10-delay), preload = True, reject_by_annotation=False)\n",
    "\n",
    "# then, looping it so more sets of epochs are appended \n",
    "\n",
    "epochs_all = my_code.create_epochs(pre_processed_datasets=pre_processed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying different cleaning & artifact detection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICA --> peak to peak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact detection/repair/deletion mix 1\n",
    "# first ICA; visualize the components\n",
    "\n",
    "my_code.apply_ica(epochs_all)\n",
    "\n",
    "    # decide which components to reject and reject them\n",
    "    \n",
    "epochs_ica = epochs_all.copy()\n",
    "ica = ICA(n_components=14, random_state=97)\n",
    "ica.fit(epochs_ica)\n",
    "ica.exclude = [1,3]\n",
    "epochs_ica = ica.apply(epochs_ica)\n",
    "\n",
    "# then peak-to-peak amplitude rejection threshold of 200 mV\n",
    "\n",
    "reject_criteria = dict(eeg=200e-6)\n",
    "\n",
    "epochs_ica_ptp_200 = epochs_ica.drop_bad(reject=reject_criteria)\n",
    "print(epochs_ica_ptp_200.drop_log)\n",
    "epochs_ica_ptp_200.plot_drop_log()\n",
    "\n",
    "# equalizing instance counts\n",
    "\n",
    "epochs_ica_ptp_200.equalize_event_counts(epochs_ica_ptp_200.event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### peak to peak --> ICA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact detection/repair/deletion mix 2\n",
    "\n",
    "# first peak-to-peak amplitude rejection threshold of 150 mV\n",
    "epochs_ptp_200 = epochs_all.copy()\n",
    "reject_criteria = dict(eeg=200e-6)\n",
    "\n",
    "epochs_ptp_200 = epochs_ptp_200.drop_bad(reject=reject_criteria)\n",
    "print(epochs_ptp_200.drop_log)\n",
    "epochs_ptp_200.plot_drop_log()\n",
    "\n",
    "# then ICA; visualize the components\n",
    "\n",
    "my_code.apply_ica(epochs_ptp_200)\n",
    "\n",
    "    # decide which components to reject and reject them\n",
    "    \n",
    "ica = ICA(n_components=14, random_state=97)\n",
    "ica.fit(epochs_ptp_200)\n",
    "ica.exclude = [1,3]\n",
    "epochs_ptp_200_ica = ica.apply(epochs_ptp_200)\n",
    "\n",
    "# equalizing instance counts\n",
    "\n",
    "epochs_ptp_200_ica.equalize_event_counts(epochs_ptp_200_ica.event_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICA --> Autoreject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact detection/repair/deletion mix 3\n",
    "\n",
    "\n",
    "# first ICA; visualize the components\n",
    "\n",
    "#my_code.apply_ica(epochs_all)\n",
    "\n",
    "    # decide which components to reject and reject them\n",
    "    \n",
    "epochs_ica = epochs_all.copy()\n",
    "ica = ICA(n_components=14, random_state=97)\n",
    "ica.fit(epochs_ica)\n",
    "ica.exclude = [1,3]\n",
    "epochs_ica = ica.apply(epochs_ica)\n",
    "\n",
    "# then Autoreject\n",
    "\n",
    "epochs_ica_autoreject = my_code.clean_epochs(epochs_all = epochs_ica)\n",
    "\n",
    "# equalizing instance counts\n",
    "\n",
    "epochs_ica_autoreject.equalize_event_counts(epochs_ica_autoreject.event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoreject --> ICA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact detection/repair/deletion mix 4\n",
    "\n",
    "# first Autoreject\n",
    "\n",
    "epochs_autoreject = epochs_all.copy()\n",
    "epochs_autoreject = my_code.clean_epochs(epochs_all = epochs_autoreject)\n",
    "\n",
    "\n",
    "# then ICA; visualize the components\n",
    "\n",
    "#my_code.apply_ica(epochs_autoreject)\n",
    "\n",
    "    # decide which components to reject and reject them\n",
    "    \n",
    "ica = ICA(n_components=14, random_state=97)\n",
    "ica.fit(epochs_autoreject)\n",
    "ica.exclude = [1,3]\n",
    "epochs_autoreject_ica = ica.apply(epochs_autoreject)\n",
    "\n",
    "# equalizing instance counts\n",
    "\n",
    "epochs_autoreject_ica.equalize_event_counts(epochs_autoreject_ica.event_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_push_power, alpha_relax_power = my_code.create_TF_object_for_plotting(8, 12, 20, epochs_ica_ptp_200[\"Push\"], epochs_ica_ptp_200[\"Relax\"])\n",
    "beta_push_power, beta_relax_power = my_code.create_TF_object_for_plotting(12, 30, 20, epochs_ica_ptp_200[\"Push\"], epochs_ica_ptp_200[\"Relax\"])\n",
    "gamma_push_power, gamma_relax_power = my_code.create_TF_object_for_plotting(30, 80, 20, epochs_ica_ptp_200[\"Push\"], epochs_ica_ptp_200[\"Relax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract power values over time across different frequencies\n",
    "# to be able to plot topomaps \n",
    "freqs = np.logspace(*np.log10([8, 12]), num=20)\n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "alpha_push_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Push\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "alpha_relax_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Relax\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "\n",
    "freqs = np.logspace(*np.log10([12, 30]), num=20)\n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "beta_push_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Push\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "beta_relax_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Relax\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "\n",
    "freqs = np.logspace(*np.log10([30, 80]), num=20)\n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "gamma_push_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Push\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "gamma_relax_power, itc = mne.time_frequency.tfr_morlet(epochs_ica_ptp_200[\"Relax\"], freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyEEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting detrended fluctuation analysis coefficients, fisher information, hurst component\n",
    "# hjorth fractal dimension, hjirth mobility, hjorth complexity, petrosian fractal dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_epochs_over_channels_push, dfa_epochs_over_channels_push, he_epochs_over_channels_push, hfd_epochs_over_channels_push, hm_epochs_over_channels_push, hc_epochs_over_channels_push, pfd_epochs_over_channels_push, fi_epochs_over_channels_push, dfa_epochs_over_channels_push, he_epochs_over_channels_push, hfd_epochs_over_channels_push, hm_epochs_over_channels_push, hc_epochs_over_channels_push, pfd_epochs_over_channels_push, all_pyeeg_features = my_code.extract_pyeegfeatures(115, epochs_ica_ptp_200[\"Push\"],epochs_ica_ptp_200[\"Relax\"])  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Spatial Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introductory explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introductory explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB values from spectrogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introductory explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration informing feature-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs_ica_ptp_200)\n",
    "print(epochs_ptp_200_ica)\n",
    "print(epochs_ica_autoreject)\n",
    "print(epochs_autoreject_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually inspecting the epochs: plotting their averages \n",
    "# need to update it for all the artifact rejection mixes\n",
    "\n",
    "push_average = epochs_ica_ptp_200[\"Push\"].average()\n",
    "relax_average = epochs_ica_ptp_200[\"Relax\"].average()\n",
    "mne.viz.plot_compare_evokeds([push_average, relax_average])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_code.plot_topomaps(alpha_push_power, alpha_relax_power,  beta_push_power, beta_relax_power,  gamma_push_power,  gamma_relax_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more time-frequency plotting\n",
    "# out of the pop up graphs we can select the channels we want to put into spectograms\n",
    "# power will be averaged over the selected channels \n",
    "\n",
    "# TO MODULARIZE THE CODE\n",
    "\n",
    "baseline=(0.5,0.5)\n",
    "\n",
    "alpha_push_power.plot_topomap(ch_type='eeg', tmin=0.5, tmax=9.5, fmin=8, fmax=12,\n",
    "                   baseline=baseline,\n",
    "                   title='Push Alpha', show=False, contours=1)\n",
    "alpha_relax_power.plot_topomap(ch_type='eeg', tmin=0.5, tmax=9.5, fmin=8, fmax=12,\n",
    "                   baseline=baseline,\n",
    "                   title='Relax Alpha', show=False, contours=1)\n",
    "\n",
    "beta_push_power.plot_topomap(ch_type='eeg', tmin=0.5, tmax=9.5, fmin=12, fmax=30,\n",
    "                   baseline=baseline,\n",
    "                   title='Push Beta', show=False, contours=1)\n",
    "beta_relax_power.plot_topomap(ch_type='eeg', tmin=0.5, tmax=9.5, fmin=12, fmax=30,\n",
    "                   baseline=baseline,\n",
    "                   title='Relax Beta', show=False, contours=1)\n",
    "\n",
    "gamma_push_power.plot_topomap(ch_type='eeg',tmin=0.5, tmax=9.5, fmin=30, fmax=80,\n",
    "                   baseline=baseline,\n",
    "                   title='Push Gamma', show=False, contours=1)\n",
    "gamma_relax_power.plot_topomap(ch_type='eeg', tmin=0.5, tmax=9.5, fmin=30, fmax=80,\n",
    "                   baseline=baseline,\n",
    "                   title='Relax Gamma', show=False, contours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking averages of frequency bands, so that we have vectors that represent epoch x channel x time per alpha, beta, gamma\n",
    "# subtracting push frequencies per frequency from relax frequencies per frequency\n",
    "# dividing an epoch into beginning, middle, end\n",
    "# taking average of those differences per beginning, middle, end to visualize where biggest frequency differences lie\n",
    "\n",
    "# TO MODULARIZE THE CODE\n",
    "\n",
    "power_push_alpha_data = alpha_push_power.data\n",
    "power_push_alpha_data_average = power_push_alpha_data.mean(axis=1)\n",
    "power_relax_alpha_data = alpha_relax_power.data\n",
    "power_relax_alpha_data_average = power_relax_alpha_data.mean(axis=1)\n",
    "\n",
    "difference_AF3_alpha = power_push_alpha_data_average[0] - power_relax_alpha_data_average[0]\n",
    "difference_AF4_alpha = power_push_alpha_data_average[13] - power_relax_alpha_data_average[13]\n",
    "difference_F7_alpha = power_push_alpha_data_average[1] - power_relax_alpha_data_average[1]\n",
    "difference_F8_alpha = power_push_alpha_data_average[12] - power_relax_alpha_data_average[12]\n",
    "difference_F3_alpha = power_push_alpha_data_average[2] - power_relax_alpha_data_average[2]\n",
    "difference_F4_alpha = power_push_alpha_data_average[11] - power_relax_alpha_data_average[11]\n",
    "difference_FC5_alpha = power_push_alpha_data_average[3] - power_relax_alpha_data_average[3]\n",
    "difference_FC6_alpha = power_push_alpha_data_average[10] - power_relax_alpha_data_average[10]\n",
    "difference_T7_alpha = power_push_alpha_data_average[4] - power_relax_alpha_data_average[4]\n",
    "difference_T8_alpha = power_push_alpha_data_average[9] - power_relax_alpha_data_average[9]\n",
    "difference_P7_alpha = power_push_alpha_data_average[5] - power_relax_alpha_data_average[5]\n",
    "difference_P8_alpha = power_push_alpha_data_average[8] - power_relax_alpha_data_average[8]\n",
    "difference_O1_alpha = power_push_alpha_data_average[6] - power_relax_alpha_data_average[6]\n",
    "difference_O2_alpha = power_push_alpha_data_average[7] - power_relax_alpha_data_average[7]\n",
    "\n",
    "power_push_beta_data = beta_push_power.data\n",
    "power_push_beta_data_average = power_push_beta_data.mean(axis=1)\n",
    "power_relax_beta_data = beta_relax_power.data\n",
    "power_relax_beta_data_average = power_relax_beta_data.mean(axis=1)\n",
    "\n",
    "difference_AF3_beta = power_push_beta_data_average[0] - power_relax_beta_data_average[0]\n",
    "difference_AF4_beta = power_push_beta_data_average[13] - power_relax_beta_data_average[13]\n",
    "difference_F7_beta = power_push_beta_data_average[1] - power_relax_beta_data_average[1]\n",
    "difference_F8_beta = power_push_beta_data_average[12] - power_relax_beta_data_average[12]\n",
    "difference_F3_beta = power_push_beta_data_average[2] - power_relax_beta_data_average[2]\n",
    "difference_F4_beta = power_push_beta_data_average[11] - power_relax_beta_data_average[11]\n",
    "difference_FC5_beta = power_push_beta_data_average[3] - power_relax_beta_data_average[3]\n",
    "difference_FC6_beta = power_push_beta_data_average[10] - power_relax_beta_data_average[10]\n",
    "difference_T7_beta = power_push_beta_data_average[4] - power_relax_beta_data_average[4]\n",
    "difference_T8_beta = power_push_beta_data_average[9] - power_relax_beta_data_average[9]\n",
    "difference_P7_beta = power_push_beta_data_average[5] - power_relax_beta_data_average[5]\n",
    "difference_P8_beta = power_push_beta_data_average[8] - power_relax_beta_data_average[8]\n",
    "difference_O1_beta = power_push_beta_data_average[6] - power_relax_beta_data_average[6]\n",
    "difference_O2_beta = power_push_beta_data_average[7] - power_relax_beta_data_average[7]\n",
    "\n",
    "power_push_gamma_data = gamma_push_power.data\n",
    "power_push_gamma_data_average = power_push_gamma_data.mean(axis=1)\n",
    "power_relax_gamma_data = gamma_relax_power.data\n",
    "power_relax_gamma_data_average = power_relax_gamma_data.mean(axis=1)\n",
    "\n",
    "difference_AF3_gamma = power_push_gamma_data_average[0] - power_relax_gamma_data_average[0]\n",
    "difference_AF4_gamma = power_push_gamma_data_average[13] - power_relax_gamma_data_average[13]\n",
    "difference_F7_gamma = power_push_gamma_data_average[1] - power_relax_gamma_data_average[1]\n",
    "difference_F8_gamma = power_push_gamma_data_average[12] - power_relax_gamma_data_average[12]\n",
    "difference_F3_gamma = power_push_gamma_data_average[2] - power_relax_gamma_data_average[2]\n",
    "difference_F4_gamma = power_push_gamma_data_average[11] - power_relax_gamma_data_average[11]\n",
    "difference_FC5_gamma = power_push_gamma_data_average[3] - power_relax_gamma_data_average[3]\n",
    "difference_FC6_gamma= power_push_gamma_data_average[10] - power_relax_gamma_data_average[10]\n",
    "difference_T7_gamma = power_push_gamma_data_average[4] - power_relax_gamma_data_average[4]\n",
    "difference_T8_gamma = power_push_gamma_data_average[9] - power_relax_gamma_data_average[9]\n",
    "difference_P7_gamma = power_push_gamma_data_average[5] - power_relax_gamma_data_average[5]\n",
    "difference_P8_gamma = power_push_gamma_data_average[8] - power_relax_gamma_data_average[8]\n",
    "difference_O1_gamma = power_push_gamma_data_average[6] - power_relax_gamma_data_average[6]\n",
    "difference_O2_gamma= power_push_gamma_data_average[7] - power_relax_gamma_data_average[7]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "##### average differences for channels and frequencies and times \n",
    "# i will divide all the time points in 3 parts, part I, part II, part III\n",
    "\n",
    "# list of difference values we have \n",
    "\n",
    "alpha_differences = [difference_AF3_alpha, difference_AF4_alpha, difference_F7_alpha, difference_F8_alpha, difference_F3_alpha, difference_F4_alpha, difference_FC5_alpha, difference_FC6_alpha, difference_T7_alpha, difference_T8_alpha, difference_P7_alpha, difference_P8_alpha, difference_O1_alpha, difference_O2_alpha]\n",
    "\n",
    "\n",
    "# allpha differences part I \n",
    "\n",
    "alpha1_differences_values = []\n",
    "\n",
    "for channel in alpha_differences:\n",
    "    value = abs(channel[slice(0,199)].mean(axis=0))\n",
    "    alpha1_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part II\n",
    "\n",
    "alpha2_differences_values = []\n",
    "\n",
    "for channel in alpha_differences:\n",
    "    value = abs(channel[slice(199,2*199)].mean(axis=0))\n",
    "    alpha2_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part III\n",
    "\n",
    "alpha3_differences_values = []\n",
    "\n",
    "for channel in alpha_differences:\n",
    "    value = abs(channel[slice(2*199,3*199)].mean(axis=0))\n",
    "    alpha3_differences_values.append(value)\n",
    "\n",
    "\n",
    "beta_differences = [difference_AF3_beta, difference_AF4_beta, difference_F7_beta, difference_F8_beta, difference_F3_beta, difference_F4_beta, difference_FC5_beta, difference_FC6_beta, difference_T7_beta, difference_T8_beta, difference_P7_beta, difference_P8_beta, difference_O1_beta, difference_O2_beta]\n",
    "\n",
    "\n",
    "# allpha differences part I \n",
    "\n",
    "beta1_differences_values = []\n",
    "\n",
    "for channel in beta_differences:\n",
    "    value = abs(channel[slice(0,199)].mean(axis=0))\n",
    "    beta1_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part II\n",
    "\n",
    "beta2_differences_values = []\n",
    "\n",
    "for channel in beta_differences:\n",
    "    value = abs(channel[slice(199,2*199)].mean(axis=0))\n",
    "    beta2_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part III\n",
    "\n",
    "beta3_differences_values = []\n",
    "\n",
    "for channel in beta_differences:\n",
    "    value = abs(channel[slice(2*199,3*199)].mean(axis=0))\n",
    "    beta3_differences_values.append(value)\n",
    "\n",
    "\n",
    "    \n",
    "gamma_differences = [difference_AF3_gamma, difference_AF4_gamma, difference_F7_gamma, difference_F8_gamma, difference_F3_gamma, difference_F4_gamma, difference_FC5_gamma, difference_FC6_gamma, difference_T7_gamma, difference_T8_gamma, difference_P7_gamma, difference_P8_gamma, difference_O1_gamma, difference_O2_gamma]\n",
    "\n",
    "\n",
    "# allpha differences part I \n",
    "\n",
    "gamma1_differences_values = []\n",
    "\n",
    "for channel in gamma_differences:\n",
    "    value = abs(channel[slice(0,199)].mean(axis=0))\n",
    "    gamma1_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part II\n",
    "\n",
    "gamma2_differences_values = []\n",
    "\n",
    "for channel in gamma_differences:\n",
    "    value = abs(channel[slice(199,2*199)].mean(axis=0))\n",
    "    gamma2_differences_values.append(value)\n",
    "    \n",
    "# allpha differences part III\n",
    "\n",
    "gamma3_differences_values = []\n",
    "\n",
    "for channel in gamma_differences:\n",
    "    value = abs(channel[slice(2*199,3*199)].mean(axis=0))\n",
    "    gamma3_differences_values.append(value)\n",
    "\n",
    "print(\"Alpha differences values in 1st chunk:\\n\", alpha1_differences_values)\n",
    "print(\"Alpha differences values in 2nd chunk:\\n\", alpha2_differences_values)\n",
    "print(\"Alpha differences values in 3rd chunk:\\n\", alpha3_differences_values)\n",
    "print(\"Beta differences values in 1st chunk:\\n\", beta1_differences_values)\n",
    "print(\"Beta differences values in 2nd chunk:\\n\", beta2_differences_values)\n",
    "print(\"Beta differences values in 3rd chunk:\\n\", beta3_differences_values)\n",
    "print(\"Gamma differences values in 1st chunk:\\n\", gamma1_differences_values)\n",
    "print(\"Gamma differences values in 2nd chunk:\\n\", gamma2_differences_values)\n",
    "print(\"Gamma differences values in 3rd chunk:\\n\", gamma3_differences_values)\n",
    "\n",
    "plt.figure(85)\n",
    "plt.plot(epochs_all.ch_names,alpha1_differences_values, label = \"alpha\")\n",
    "plt.plot(epochs_all.ch_names,beta1_differences_values, label = \"beta\")\n",
    "plt.plot(epochs_all.ch_names,gamma1_differences_values, label = \"gamma\")\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Power difference\")\n",
    "plt.suptitle('Average differences in push/relax in 1 s - 3.34 s ', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "plt.figure(86)\n",
    "plt.plot(epochs_all.ch_names,alpha2_differences_values, label = \"alpha\")\n",
    "plt.plot(epochs_all.ch_names,beta2_differences_values, label = \"beta\")\n",
    "plt.plot(epochs_all.ch_names,gamma2_differences_values, label = \"gamma\")\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Power difference\")\n",
    "plt.suptitle('Average differences in push/relax in 3.34 s - 5.67 s ', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "plt.figure(87)\n",
    "plt.plot(epochs_all.ch_names,alpha3_differences_values, label = \"alpha\")\n",
    "plt.plot(epochs_all.ch_names,beta3_differences_values, label = \"beta\")\n",
    "plt.plot(epochs_all.ch_names,gamma3_differences_values, label = \"gamma\")\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Power difference\")\n",
    "plt.suptitle('Average differences in push/relax in 5.67 s - 8s ', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Sliding estimator on raw data and cropping epochs based on that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting alpha, beta, gamma frequencies and their mixes per channel over time \n",
    "# repeat for every artifact rejection mix \n",
    "\n",
    "freqs, alpha_push_power, alpha_push_itc, instances = my_code.create_time_frequency_matrices(8, 12, 20, 115, 14, 769, epochs_ica_ptp_200[\"Push\"])\n",
    "freqs, alpha_relax_power, alpha_relax_itc, instances = my_code.create_time_frequency_matrices(8, 12, 20, 115, 14, 769, epochs_ica_ptp_200[\"Relax\"])\n",
    "freqs, beta_push_power, beta_push_itc, instances = my_code.create_time_frequency_matrices(13, 30, 20, 115, 14, 769, epochs_ica_ptp_200[\"Push\"])\n",
    "freqs, beta_relax_power, beta_relax_itc, instances = my_code.create_time_frequency_matrices(13, 30, 20, 115, 14, 769, epochs_ica_ptp_200[\"Relax\"])\n",
    "freqs, gamma_push_power, gamma_push_itc, instances = my_code.create_time_frequency_matrices(30, 80, 20, 115, 14, 769, epochs_ica_ptp_200[\"Push\"])\n",
    "freqs, gamma_relax_power, gamma_relax_itc, instances = my_code.create_time_frequency_matrices(30, 80, 20, 115, 14, 769, epochs_ica_ptp_200[\"Relax\"])\n",
    "alpha_beta_push_power = np.concatenate((alpha_push_power, beta_push_power), axis=3)\n",
    "alpha_beta_relax_power = np.concatenate((alpha_relax_power, beta_relax_power), axis=3)\n",
    "alpha_beta_gamma_push_power = np.concatenate((alpha_beta_push_power, gamma_push_power), axis=3)\n",
    "alpha_beta_gamma_relax_power = np.concatenate((alpha_beta_relax_power, gamma_relax_power), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running a series of SlidingEstimators to see where the different frequencies achieve biggest accuracy\n",
    "\n",
    "alpha_push_power_channels, alpha_relax_power_channels, mean_scores_alpha = my_code.sliding_estimator(alpha_push_power, alpha_relax_power, 115, 769)\n",
    "beta_push_power_channels, beta_relax_power_channels, mean_scores_beta = my_code.sliding_estimator(beta_push_power, beta_relax_power, 115, 769)\n",
    "gamma_push_power_channels, gamma_relax_power_channels, mean_scores_gamma = my_code.sliding_estimator(gamma_push_power, gamma_relax_power, 115, 769)\n",
    "\n",
    "# repeat for every artifact rejection mix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropping data into different times & channels based on SlidingEstimator and visual inspection of my method\n",
    "\n",
    "# based on LDA on frequency data from all epoch (choosing timepoints that gave SE an accuracy over a certain threshold)\n",
    "\n",
    "res_alpha = []\n",
    "for idx in range(0, len(mean_scores_alpha)) :\n",
    "    if mean_scores_alpha[idx] > 0.60:\n",
    "        res_alpha.append(idx)\n",
    "\n",
    "samples_alpha_push = alpha_push_power_channels[:,:,res_alpha]\n",
    "samples_alpha_relax = alpha_push_power_channels[:,:,res_alpha]\n",
    "\n",
    "# based on my own visual inspection\n",
    "\n",
    "    #visualinspection_alpha_push = alpha_push_power_channels[:,:,my_timepoints]\n",
    "    #visualinspection_alpha_relax = alpha_push_power_channels[:,:,my_timepoints]\n",
    "\n",
    "# cropping channels based on my own visual inspection \n",
    "\n",
    "    # e.g. choosing only O2, F8, F7, AF4, P7 for alpha frequencies; channel indices: O2 - 7, F8 - 12, F7 - 1, AF3 - 13, P7 - 5\n",
    "    \n",
    "\n",
    "alpha_push_power_croppedchannels = my_code.select_channels((7,12,1,13,5), alpha_push_power, 115, 769)\n",
    "alpha_relax_power_croppedchannels = my_code.select_channels((7,12,1,13,5), alpha_relax_power, 115, 769)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying different combinations of the TF data by LDA, SVM and DL architectures\n",
    "\n",
    "# alpha frequencies from different artifact mixes \n",
    "\n",
    "    # e.g. \n",
    "\n",
    "alpha_mix1_data = my_code.run_lda(alpha_push_power, alpha_relax_power, 115)\n",
    "alpha_mix1_data = my_code.run_svm(alpha_push_power, alpha_relax_power, 115)\n",
    "\n",
    "# beta frequencies from different artifact mixes \n",
    "\n",
    "    # e.g. \n",
    "\n",
    "beta_mix1_data = my_code.run_lda(beta_push_power, beta_relax_power, 115)\n",
    "beta_mix1_data = my_code.run_svm(beta_push_power, beta_relax_power, 115)\n",
    "\n",
    "# gamma frequencies from different artifact mixes \n",
    "\n",
    "    # e.g. \n",
    "\n",
    "gamma_mix1_data = my_code.run_lda(gamma_push_power, gamma_relax_power, 115)\n",
    "gamma_mix1_data = my_code.run_svm(gamma_push_power, gamma_relax_power, 115)\n",
    "\n",
    "# alpha&beta frequencies from different artifact mixes \n",
    "\n",
    "    # e.g. \n",
    "\n",
    "alpha_beta_mix1_data = my_code.run_lda(alpha_beta_push_power, alpha_beta_relax_power, 115)\n",
    "alpha_beta_mix1_data = my_code.run_svm(alpha_beta_push_power, alpha_beta_relax_power, 115)\n",
    "\n",
    "# alpha, beta&gamma frequencies from different artifact mixes \n",
    "\n",
    "    # e.g. \n",
    "\n",
    "alpha_beta_gamma_mix1_data = my_code.run_lda(alpha_beta_gamma_push_power, alpha_beta_gamma_relax_power, 115)\n",
    "alpha_beta_gamma_mix1_data = my_code.run_svm(alpha_beta_gamma_push_power, alpha_beta_gamma_relax_power, 115)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification of alpha frequencies at timpoints where SE accuracy was higher than 65%\n",
    "\n",
    "alpha_samples_mix1_data = my_code.run_lda(samples_alpha_push, samples_alpha_relax, 115)\n",
    "alpha_samples_mix1_data = my_code.run_svm(samples_alpha_push, samples_alpha_relax, 115)\n",
    "\n",
    "# classification of alpha frequencies from different channels based on visual inspection \n",
    "\n",
    "alpha_cropped_channels_mix1_data = my_code.run_lda(alpha_push_power_croppedchannels, alpha_relax_power_croppedchannels, 115)\n",
    "alpha_cropped_channels_mix1_data = my_code.run_svm(alpha_push_power_croppedchannels, alpha_relax_power_croppedchannels, 115)\n",
    "\n",
    "\n",
    "# repeat those combination for the rest of frequency bands and the rest of artifact rejection mixes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pyEEG module to extract the features\n",
    "# TO MODULARIZE THE CODE\n",
    "\n",
    "\n",
    "import pyeeg\n",
    "\n",
    "channels_list = np.arange(14)\n",
    "channels_list\n",
    "instances = np.arange(115)\n",
    "print(instances.shape)\n",
    "\n",
    "push_epochs_data = epochs_all[\"Push\"]\n",
    "relax_epochs_data = epochs_all[\"Relax\"]\n",
    "\n",
    "push_epochs_data = push_epochs_data.get_data()\n",
    "relax_epochs_data = relax_epochs_data.get_data()\n",
    "\n",
    "labels = np.concatenate((np.ones(shape=(115)), np.zeros(shape=(115))), axis = 0)\n",
    "\n",
    "\n",
    "#### Extracting DFA parameters\n",
    "\n",
    "dfa_per_channel_push = np.zeros(shape=(14))\n",
    "dfa_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "dfa_per_channel_relax = np.zeros(shape=(14))\n",
    "dfa_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        dfa_push = pyeeg.dfa(push_epochs_data[instance, channel])\n",
    "        dfa_per_channel_push[channel]=dfa_push\n",
    "        dfa_relax = pyeeg.dfa(relax_epochs_data[instance, channel])\n",
    "        dfa_per_channel_relax[channel]=dfa_relax\n",
    "    \n",
    "    dfa_epochs_over_channels_push[instance] = dfa_per_channel_push\n",
    "    dfa_epochs_over_channels_relax[instance] = dfa_per_channel_relax\n",
    "\n",
    "print(dfa_epochs_over_channels_push.shape)\n",
    "print(dfa_epochs_over_channels_relax.shape)\n",
    "\n",
    "#### Extracting Fisher Information\n",
    "\n",
    "fi_per_channel_push = np.zeros(shape=(14))\n",
    "fi_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "fi_per_channel_relax = np.zeros(shape=(14))\n",
    "fi_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        fi_push = pyeeg.fisher_info(push_epochs_data[instance, channel],1, 2)\n",
    "        fi_per_channel_push[channel]=fi_push\n",
    "        fi_relax = pyeeg.fisher_info(relax_epochs_data[instance, channel], 1,2)\n",
    "        fi_per_channel_relax[channel]=fi_relax\n",
    "    \n",
    "    fi_epochs_over_channels_push[instance] =  fi_per_channel_push\n",
    "    fi_epochs_over_channels_relax[instance] =  fi_per_channel_relax\n",
    "\n",
    "print(fi_epochs_over_channels_push.shape)\n",
    "print(fi_epochs_over_channels_relax.shape)\n",
    "\n",
    "#### Extracting Hurst Exponent\n",
    "\n",
    "pyeeg.hurst(push_epochs_data[6, 4])\n",
    "\n",
    "he_per_channel_push = np.zeros(shape=(14))\n",
    "he_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "he_per_channel_relax = np.zeros(shape=(14))\n",
    "he_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        he_push = pyeeg.hurst(push_epochs_data[instance, channel])\n",
    "        he_per_channel_push[channel]=he_push\n",
    "        he_relax = pyeeg.hurst(push_epochs_data[instance, channel])\n",
    "        he_per_channel_relax[channel]=he_relax\n",
    "    \n",
    "    he_epochs_over_channels_push[instance] =  he_per_channel_push\n",
    "    he_epochs_over_channels_relax[instance] =  he_per_channel_relax\n",
    "\n",
    "print(he_epochs_over_channels_push.shape)\n",
    "print(he_epochs_over_channels_relax.shape)\n",
    "\n",
    "#### Extracting Hjorth Fractal Dimension \n",
    "\n",
    "### according to a paper saved in my feature extraction folder, value of 18 is best for kmax argument of hfd\n",
    "\n",
    "pyeeg.hfd(push_epochs_data[6, 4],18)\n",
    "\n",
    "hfd_per_channel_push = np.zeros(shape=(14))\n",
    "hfd_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "hfd_per_channel_relax = np.zeros(shape=(14))\n",
    "hfd_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        hfd_push = pyeeg.hfd(push_epochs_data[instance, channel], 18)\n",
    "        hfd_per_channel_push[channel]=hfd_push\n",
    "        hfd_relax = pyeeg.hfd(push_epochs_data[instance, channel], 18)\n",
    "        hfd_per_channel_relax[channel]=hfd_relax\n",
    "    \n",
    "    hfd_epochs_over_channels_push[instance] =  hfd_per_channel_push\n",
    "    hfd_epochs_over_channels_relax[instance] =  hfd_per_channel_relax\n",
    "\n",
    "print(hfd_epochs_over_channels_push.shape)\n",
    "print(hfd_epochs_over_channels_relax.shape)\n",
    "\n",
    "#### Extracting Hjorth Mobility & Complexity\n",
    "\n",
    "\n",
    "# first is mobility\n",
    "# second is complexity\n",
    "\n",
    "hm_per_channel_push = np.zeros(shape=(14))\n",
    "hm_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "hc_per_channel_push = np.zeros(shape=(14))\n",
    "hc_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "hm_per_channel_relax = np.zeros(shape=(14))\n",
    "hm_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "hc_per_channel_relax = np.zeros(shape=(14))\n",
    "hc_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        hmc_push = pyeeg.hjorth(push_epochs_data[instance, channel])\n",
    "        hm_per_channel_push[channel]=hmc_push[0]\n",
    "        hc_per_channel_push[channel]=hmc_push[1]\n",
    "        hmc_relax = pyeeg.hjorth(push_epochs_data[instance, channel])\n",
    "        hm_per_channel_relax[channel]=hmc_relax[0]\n",
    "        hc_per_channel_relax[channel]=hmc_relax[1]\n",
    "    \n",
    "    hm_epochs_over_channels_push[instance] =  hm_per_channel_push\n",
    "    hc_epochs_over_channels_push[instance] =  hc_per_channel_push\n",
    "    hm_epochs_over_channels_relax[instance] =  hm_per_channel_relax\n",
    "    hc_epochs_over_channels_relax[instance] =  hc_per_channel_relax\n",
    "\n",
    "print(hm_epochs_over_channels_push.shape)\n",
    "print(hc_epochs_over_channels_relax.shape)\n",
    "print(hm_epochs_over_channels_push.shape)\n",
    "print(hc_epochs_over_channels_relax.shape)\n",
    "\n",
    "#### Extracting Petrosian Fractal Dimension\n",
    "\n",
    "\n",
    "pfd_per_channel_push = np.zeros(shape=(14))\n",
    "pfd_epochs_over_channels_push = np.zeros(shape=(115, 14))\n",
    "pfd_per_channel_relax = np.zeros(shape=(14))\n",
    "pfd_epochs_over_channels_relax = np.zeros(shape=(115, 14))\n",
    "\n",
    "for instance in instances:\n",
    "    \n",
    "    for channel in channels_list:\n",
    "        pfd_push = pyeeg.pfd(push_epochs_data[instance, channel])\n",
    "        pfd_per_channel_push[channel]=pfd_push\n",
    "        pfd_relax = pyeeg.pfd(push_epochs_data[instance, channel])\n",
    "        pfd_per_channel_relax[channel]=pfd_relax\n",
    "    \n",
    "    pfd_epochs_over_channels_push[instance] =  pfd_per_channel_push\n",
    "    pfd_epochs_over_channels_relax[instance] =  pfd_per_channel_relax\n",
    "\n",
    "print(pfd_epochs_over_channels_push.shape)\n",
    "print(pfd_epochs_over_channels_relax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying different combinations of the pyEEG features by LDA, SVM and DL architectures\n",
    "\n",
    "### Running those PyEEG features through LDA\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    scores = []\n",
    "    all_data = all_pyeeg_features\n",
    "    all_data_train = all_pyeeg_features.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "# Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "### Running feature-by-feature through LDA \n",
    "\n",
    "fi_epochs_over_channels = np.concatenate((fi_epochs_over_channels_push, fi_epochs_over_channels_relax), axis = 0)\n",
    "dfa_epochs_over_channels = np.concatenate((dfa_epochs_over_channels_push, dfa_epochs_over_channels_relax), axis = 0)\n",
    "he_epochs_over_channels = np.concatenate((he_epochs_over_channels_push, he_epochs_over_channels_relax), axis = 0)\n",
    "hfd_epochs_over_channels = np.concatenate((hfd_epochs_over_channels_push, hfd_epochs_over_channels_relax), axis = 0)\n",
    "hm_epochs_over_channels = np.concatenate((hm_epochs_over_channels_push, hm_epochs_over_channels_relax), axis = 0)\n",
    "hc_epochs_over_channels = np.concatenate((hc_epochs_over_channels_push, hc_epochs_over_channels_relax), axis = 0)\n",
    "pfd_epochs_over_channels = np.concatenate((pfd_epochs_over_channels_push, pfd_epochs_over_channels_relax), axis = 0)\n",
    "features_list = fi_epochs_over_channels, dfa_epochs_over_channels, he_epochs_over_channels, hfd_epochs_over_channels, hm_epochs_over_channels, hc_epochs_over_channels, pfd_epochs_over_channels\n",
    "\n",
    "for item in features_list:     \n",
    "    \n",
    "    push = np.ones(shape=(121))\n",
    "    relax = np.zeros(shape=(121))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    \n",
    "    # Now, we run the alpha data through LDA\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    scores = []\n",
    "    all_data = item\n",
    "    all_data_train = item.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "# Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "### Running those PyEEG features through SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrying out Principal Comonent Analysis (PCA) to extract features\n",
    "\n",
    "# TO FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying PCA features by LDA, SVM and DL architectures\n",
    "\n",
    "# TO FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrying out Common Spatial Patterns (CSP) to extract features & classifying CSP features by LDA, SVM and DL architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "# just alpha\n",
    "alpha_score = my_code.csp_lda(alpha_push_power, alpha_relax_power, 121)\n",
    "# just beta\n",
    "beta_score = my_code.csp_lda(beta_push_power, beta_relax_power, 121)\n",
    "# just gamma\n",
    "gamma_score = my_code.csp_lda(gamma_push_power, gamma_relax_power, 121)\n",
    "# alpha & beta\n",
    "alpha_beta_score = my_code.csp_lda(alpha_beta_push_power, alpha_beta_relax_power, 121)\n",
    "\n",
    "# alpha, beta & gamma\n",
    "\n",
    "alpha_beta_gamma_score = my_code.csp_lda(alpha_beta_gamma_push_power, alpha_beta_gamma_relax_power, 121)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying combinations of TF, PyEEG, PCA & CSP features by LDA, SVM and DL architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_powers = np.concatenate((alpha_push_power, alpha_push_power), axis = 0)\n",
    "alpha_powers.shape\n",
    "\n",
    "alpha_powers=alpha_powers.mean(axis=2)\n",
    "\n",
    "alpha_powers.shape\n",
    "\n",
    "alpha_powers = alpha_powers.reshape(242, -1)\n",
    "\n",
    "alpha_powers.shape\n",
    "\n",
    "alpha_fi_vectors = np.concatenate((alpha_powers, fi_epochs_over_channels), axis = 1)\n",
    "alpha_fi_vectors.shape\n",
    "\n",
    " \n",
    "    \n",
    "    push = np.ones(shape=(121))\n",
    "    relax = np.zeros(shape=(121))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    \n",
    "    # Now, we run the alpha data through LDA\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    scores = []\n",
    "    all_data = alpha_fi_vectors\n",
    "    all_data_train = alpha_fi_vectors.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "# Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "### Combining Fisher Information with gamma vectors\n",
    "\n",
    "gamma_powers = np.concatenate((gamma_push_power, gamma_push_power), axis = 0)\n",
    "gamma_powers.shape\n",
    "\n",
    "gamma_powers=gamma_powers.mean(axis=2)\n",
    "\n",
    "gamma_powers.shape\n",
    "\n",
    "gamma_powers = gamma_powers.reshape(242, -1)\n",
    "\n",
    "gamma_powers.shape\n",
    "\n",
    "gamma_fi_vectors = np.concatenate((gamma_powers, fi_epochs_over_channels), axis = 1)\n",
    "gamma_fi_vectors.shape\n",
    "\n",
    " \n",
    "    \n",
    "    push = np.ones(shape=(121))\n",
    "    relax = np.zeros(shape=(121))\n",
    "    labels = np.concatenate((push ,relax), axis = 0)\n",
    "    \n",
    "    \n",
    "    # Now, we run the alpha data through LDA\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "    scores = []\n",
    "    all_data = alpha_fi_vectors\n",
    "    all_data_train = gamma_fi_vectors.copy()\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(all_data_train)\n",
    "\n",
    "# Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('LDA', lda)])\n",
    "    scores = cross_val_score(clf, all_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying raw data & spectograms by DL architectures \n",
    "\n",
    "# TO FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons section\n",
    "\n",
    "# TO START"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
